{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd95d8d",
   "metadata": {},
   "source": [
    "# Lectures on Machine Learning for Strong Gravity\n",
    "## Lecture 2: Autoencoders and Generative Adversarial Networks\n",
    "\n",
    "To test it, simply press Ctrl+Enter sequentially in each cell, or click on the small icons on the left with the \"play\" symbol.\n",
    "\n",
    "<br>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/raimonluna/MachineLearningForStrongGravity/blob/main/Lecture2_Autoencoders_and_Generative_Adversarial_Networks.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "### In this lecture, you will learn:\n",
    " - How to reduce the dimensionality of data by finding an appropriate basis of vectors.\n",
    " - How autoencoders \"compress\" the data by automatically identifying the governing parameters.\n",
    " - How we can use GANs to generate samples from the same distribution as the training data.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd08143",
   "metadata": {
    "id": "3dd08143"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab369ec",
   "metadata": {
    "id": "7ab369ec"
   },
   "source": [
    "# 1. Toy Model Waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30259af3",
   "metadata": {},
   "source": [
    "Let's first create a dataset of toy \"waveforms\" of the form\n",
    "\n",
    "$$\\psi = \\sin(2 \\pi k x) e^{-8x^2}.$$\n",
    "\n",
    "we will use these as examples to train our models.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_some_cases(dataset1, dataset2 = None):\n",
    "    fig, ax = plt.subplots(2,3, figsize = (12, 4));\n",
    "    font = {'size'   : 10}\n",
    "    plt.rc('font', **font)\n",
    "\n",
    "    x = np.linspace(-1, 1, 100, dtype = np.float32)\n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            if dataset2 is not None:\n",
    "                ax[i,j].plot(x, dataset1[j + 3*i, :], 'b', label = 'Data')\n",
    "                ax[i,j].plot(x, dataset2[j + 3*i, :], 'r--', label = 'Approx')\n",
    "                ax[i,j].legend(loc = 'upper left')\n",
    "            else:\n",
    "                ax[i,j].plot(x, dataset1[j + 3*i, :], 'b')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af372568",
   "metadata": {
    "id": "af372568",
    "outputId": "9b991cb3-561b-482a-9743-ea1d6cf1bbbf"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x       = np.linspace(-1, 1, 100, dtype = np.float32)\n",
    "randoms = 5*np.random.rand(1000)\n",
    "data    = torch.tensor(np.array([    np.sin(2 * k * np.pi * x) * np.exp(- 8 * x**2 ) for k in randoms]))\n",
    "\n",
    "show_some_cases(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30c62d",
   "metadata": {},
   "source": [
    "# 2. Principal Component Analysis / Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232013bd",
   "metadata": {},
   "source": [
    "First we use linear algebra algorithms to express the waveforms as a linear combination of a few wisely chosen vectors:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e758f382",
   "metadata": {
    "id": "e758f382",
    "outputId": "57c7624e-8d41-4839-f890-5eafd2c51272"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 10)\n",
    "pca.fit(data)\n",
    "\n",
    "encoded = pca.transform(data)\n",
    "decoded = pca.inverse_transform(encoded)\n",
    "\n",
    "show_some_cases(data, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa57f2",
   "metadata": {},
   "source": [
    "What are the first representative vectors?\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733486e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_some_cases(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd732d",
   "metadata": {},
   "source": [
    "And what is their importance representing the data?\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e87768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale('log')\n",
    "plt.plot(pca.singular_values_, '.');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81029541",
   "metadata": {},
   "source": [
    "## Challenges!\n",
    " - Try other algorithms, such as TruncatedSVD instead of PCA. What changes do you see?\n",
    " - How much can you reduce the number of components before the approximation gets bad?\n",
    " \n",
    " <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a21bc",
   "metadata": {},
   "source": [
    "# 3. Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac029665",
   "metadata": {},
   "source": [
    "An autoencoder introduces a bottleneck in a neural network, so the model is forced to \"summarize\" the data in a few parameters. The result is similar to PCA/SVD, but it can cpture nonlinear dependences.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64443f63",
   "metadata": {
    "id": "64443f63"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(100, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 2)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 100)\n",
    "        )\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        for m in self.encoder.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean = 0, std = 0.01)\n",
    "                nn.init.constant_(m.bias, val = 0.0)\n",
    "    \n",
    "        for m in self.decoder.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean = 0, std = 0.01)\n",
    "                nn.init.constant_(m.bias, val = 0.0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599b8df",
   "metadata": {},
   "source": [
    "## 3. 1. Training with 1-parameter dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f2790",
   "metadata": {
    "id": "813f2790",
    "outputId": "755dcb3a-210f-4de1-fe95-99d22647e239"
   },
   "outputs": [],
   "source": [
    "auto_encoder  = AutoEncoder()\n",
    "\n",
    "loss_hist = []\n",
    "\n",
    "optimizer = optim.Adam(auto_encoder.parameters(), lr=0.001)\n",
    "\n",
    "data_loader = DataLoader(data,  batch_size=10, num_workers=8, pin_memory=True)\n",
    "\n",
    "################## Training and Plotting ##################\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 5));\n",
    "font = {'size'   : 19}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "for epoch in range(30):\n",
    "    try:\n",
    "        for X in data_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            reconstructed = auto_encoder(X)\n",
    "            loss = torch.sum((X - reconstructed)**2)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        loss_hist.append(loss.item())\n",
    "        latent = auto_encoder.encoder(data).detach().numpy()\n",
    "\n",
    "        ax1.cla()\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.set_xlabel('epoch')\n",
    "        ax1.set_ylabel('loss')\n",
    "        ax1.plot(loss_hist)\n",
    "\n",
    "        ax2.cla()\n",
    "        ax2.set_xlabel('x')\n",
    "        ax2.set_ylabel('y')\n",
    "        ax2.scatter(latent[:,0], latent[:,1], c = randoms)\n",
    "\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        plt.tight_layout()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a832a",
   "metadata": {
    "id": "cc5a832a",
    "outputId": "cf206cc6-9ff6-4028-92f7-0006ea4c2973"
   },
   "outputs": [],
   "source": [
    "encoded = auto_encoder.encoder(data)\n",
    "decoded = auto_encoder.decoder(encoded)\n",
    "\n",
    "show_some_cases(data, decoded.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d3c66",
   "metadata": {},
   "source": [
    "## 3.2. Let's try a 2-parameter dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144aeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x        = np.linspace(-1, 1, 100, dtype = np.float32)\n",
    "randoms1 = 5*np.random.rand(1000)\n",
    "randoms2 = 2*np.random.rand(1000)\n",
    "data     = torch.tensor(np.array([    np.sin(2 * k1 * np.pi * x) * np.exp(- k2 * 8 * x**2 ) \\\n",
    "                                  for k1, k2 in zip(randoms1, randoms2)]))\n",
    "\n",
    "show_some_cases(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder  = AutoEncoder()\n",
    "\n",
    "loss_hist = []\n",
    "\n",
    "optimizer = optim.Adam(auto_encoder.parameters(), lr=0.001)\n",
    "\n",
    "data_loader = DataLoader(data,  batch_size=10, num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "################## Training and Plotting ##################\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (12, 4));\n",
    "font = {'size'   : 19}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "for epoch in range(100):\n",
    "    try:\n",
    "        for X in data_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            reconstructed = auto_encoder(X)\n",
    "            loss = torch.sum((X - reconstructed)**2)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        loss_hist.append(loss.item())\n",
    "        latent = auto_encoder.encoder(data).detach().numpy()\n",
    "\n",
    "        ax1.cla()\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.set_xlabel('epoch')\n",
    "        ax1.set_ylabel('loss')\n",
    "        ax1.plot(loss_hist)\n",
    "\n",
    "        ax2.cla()\n",
    "        ax2.set_xlabel('x')\n",
    "        ax2.set_ylabel('y')\n",
    "        ax2.scatter(latent[:,0], latent[:,1], c = randoms1)\n",
    "        \n",
    "        ax3.cla()\n",
    "        ax3.set_xlabel('x')\n",
    "        ax3.set_ylabel('y')\n",
    "        ax3.scatter(latent[:,0], latent[:,1], c = randoms2)\n",
    "\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        plt.tight_layout()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a151f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = auto_encoder.encoder(data)\n",
    "decoded = auto_encoder.decoder(encoded)\n",
    "\n",
    "show_some_cases(data, decoded.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df8e10",
   "metadata": {
    "id": "97df8e10"
   },
   "source": [
    "# 4. Generative Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71338e41",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks try to create samples with the same probability distribution as our dataset. They have two parts:\n",
    "\n",
    " - The Generator: Takes a vector of normally distributed random values, and tries to translate them to convincing fake data.\n",
    " - The Discriminator: Tries to discriminate between real and fake data.\n",
    "\n",
    "During the training, the generator learns to make better and better data. meanwhile, the discriminator becomes better and better at identifying the false data from the generator.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb3baf",
   "metadata": {
    "id": "afeb3baf"
   },
   "outputs": [],
   "source": [
    "X = torch.normal(0., 1., (1000, 2))\n",
    "A = torch.tensor([[1, 2], [-0.1, 0.5]])\n",
    "b = torch.tensor([1, 2])\n",
    "data = torch.matmul(X, A) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ccacf8",
   "metadata": {
    "id": "b5ccacf8"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2,2),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean = 0, std = 0.02)\n",
    "                nn.init.constant_(m.bias, val = 0.0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 5),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(5,3),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(3,1),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean = 0, std = 0.02)\n",
    "                nn.init.constant_(m.bias, val = 0.0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20e9d2",
   "metadata": {
    "id": "6f20e9d2",
    "outputId": "4e6ad57e-eedc-453a-9bc8-ef900cf25e1f"
   },
   "outputs": [],
   "source": [
    "generator     = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "gen_loss_hist = []\n",
    "dis_loss_hist = []\n",
    "\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr=0.005)\n",
    "dis_optimizer = optim.Adam(discriminator.parameters(), lr=0.05)\n",
    "\n",
    "data_loader = DataLoader(data,  batch_size=8, num_workers=8, pin_memory=True)\n",
    "\n",
    "################## Training and Plotting ##################\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 4));\n",
    "font = {'size'   : 19}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "for epoch in range(20):\n",
    "    try:\n",
    "        for X in data_loader:\n",
    "            loss_function = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "            latent_space_points = torch.normal(0, 1, size=(8, 2))\n",
    "\n",
    "            ################## Train the Discriminator ##################\n",
    "            dis_optimizer.zero_grad()\n",
    "            fake_data = generator(latent_space_points)\n",
    "\n",
    "            real_data_evaluation = discriminator(X)\n",
    "            fake_data_evaluation = discriminator(fake_data.detach())\n",
    "\n",
    "            real_data_loss = loss_function(real_data_evaluation, torch.ones(8, 1))\n",
    "            fake_data_loss = loss_function(fake_data_evaluation, torch.zeros(8, 1))\n",
    "\n",
    "            dis_loss = (real_data_loss + fake_data_loss)/2\n",
    "            \n",
    "            dis_loss.backward()\n",
    "            dis_optimizer.step()\n",
    "            #############################################################\n",
    "\n",
    "            #################### Train the Generator ####################\n",
    "            gen_optimizer.zero_grad()\n",
    "            fake_data = generator(latent_space_points)\n",
    "\n",
    "            fake_data_evaluation = discriminator(fake_data)\n",
    "            gen_loss = loss_function(fake_data_evaluation, torch.ones(8, 1))\n",
    "\n",
    "            gen_loss.backward()\n",
    "            gen_optimizer.step()                             \n",
    "            #############################################################\n",
    "        \n",
    "        dis_loss_hist.append(dis_loss.item())\n",
    "        gen_loss_hist.append(gen_loss.item())\n",
    "        \n",
    "        latent_space_points = torch.normal(0, 1, size=(1000, 2))\n",
    "        fake_data = generator(latent_space_points).cpu().detach().numpy()\n",
    "        real_data = data\n",
    "\n",
    "        ax1.cla()\n",
    "        ax1.set_xlabel('epoch')\n",
    "        ax1.set_ylabel('loss')\n",
    "        ax1.plot(gen_loss_hist)\n",
    "        ax1.plot(dis_loss_hist)\n",
    "\n",
    "        ax2.cla()\n",
    "        ax2.set_xlabel('x')\n",
    "        ax2.set_ylabel('y')\n",
    "        ax2.scatter(real_data[:, 0], real_data[:, 1])\n",
    "        ax2.scatter(fake_data[:, 0], fake_data[:, 1])\n",
    "\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        plt.tight_layout()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1effcc6",
   "metadata": {
    "id": "c1effcc6"
   },
   "source": [
    "# 5. Deep Convolutional GAN for waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c9b90",
   "metadata": {},
   "source": [
    "Let's try to create fake waveforms! In this case we need more complex neural networks (convolutional), so please enable a GPU on Google Colab by going to \"Edit\" > \"Notebook settings\" and select “GPU”. You may need to restart the kernel and run the first cell with the libraries.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9347ee13",
   "metadata": {
    "id": "9347ee13"
   },
   "outputs": [],
   "source": [
    "wave_size  = 64\n",
    "batch_size = 64\n",
    "lr         = 0.0002\n",
    "beta1      = 0.5\n",
    "niter      = 25\n",
    "\n",
    "nz  = 100   # Size of latent vector\n",
    "ngf = 64    # Filter size of generator\n",
    "ndf = 64    # Filter size of discriminator\n",
    "nc  = 1     # Output wave channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Op49bBqoufIy",
   "metadata": {
    "id": "Op49bBqoufIy"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "def show_some_cases(dataset):\n",
    "    fig, ax = plt.subplots(2,3, figsize = (12, 4));\n",
    "    font = {'size'   : 12}\n",
    "    plt.rc('font', **font)\n",
    "\n",
    "    x = np.linspace(-1, 1, 64, dtype = np.float32)\n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            ax[i,j].plot(x, dataset[j + 3*i, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d04b7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "47d04b7b",
    "outputId": "d405bc15-b747-4087-9269-e01f362e3aab"
   },
   "outputs": [],
   "source": [
    "x       = np.linspace(-1, 1, 64, dtype = np.float32)\n",
    "randoms = 5 * np.random.rand(1024)\n",
    "data    = torch.tensor(np.array([    np.sin(2 * k * np.pi * x) * np.exp(- 8 * x**2 ) for k in randoms])).reshape(1024, 1, 64)\n",
    "\n",
    "show_some_cases(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6fb79",
   "metadata": {
    "id": "b6e6fb79"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose1d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm1d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose1d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose1d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose1d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose1d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.net(input)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv1d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv1d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv1d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv1d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv1d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.net(input)\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c65e9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38c65e9e",
    "outputId": "1808a3c5-f649-461b-bd4c-5f1d20c10168"
   },
   "outputs": [],
   "source": [
    "generator     = Generator().cuda()\n",
    "discriminator = Discriminator().cuda()\n",
    "\n",
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr, betas=(beta1, 0.999))\n",
    "dis_optimizer = optim.Adam(discriminator.parameters(), lr, betas=(beta1, 0.999))\n",
    "loss_function = nn.BCELoss().cuda()\n",
    "\n",
    "data_loader   = DataLoader(data, batch_size = batch_size, shuffle=True)\n",
    "gen_loss_hist = []\n",
    "dis_loss_hist = []\n",
    "\n",
    "for epoch in range(niter):\n",
    "    print(epoch, end = ' ')\n",
    "    for i, X in enumerate(data_loader, 0):\n",
    "        \n",
    "        X_cuda = X.cuda()\n",
    "        latent_space_points = torch.normal(0, 1, size = (batch_size, nz, 1)).cuda()\n",
    "        \n",
    "        ################## Train the Discriminator ##################\n",
    "        \n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        fake_data = generator(latent_space_points)\n",
    "\n",
    "        real_data_evaluation = discriminator(X_cuda)\n",
    "        fake_data_evaluation = discriminator(fake_data.detach())\n",
    "\n",
    "        real_data_loss = loss_function(real_data_evaluation, torch.ones(batch_size).cuda())\n",
    "        fake_data_loss = loss_function(fake_data_evaluation, torch.zeros(batch_size).cuda())\n",
    "\n",
    "        dis_loss = (real_data_loss + fake_data_loss) / 2\n",
    "\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "\n",
    "        #############################################################\n",
    "\n",
    "        #################### Train the Generator ####################\n",
    "        \n",
    "        gen_optimizer.zero_grad()\n",
    "        fake_data = generator(latent_space_points)\n",
    "\n",
    "        fake_data_evaluation = discriminator(fake_data)\n",
    "        gen_loss = loss_function(fake_data_evaluation, torch.ones(batch_size).cuda())\n",
    "\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()  \n",
    "\n",
    "        #############################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e592ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gen_loss_hist)\n",
    "plt.plot(dis_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c4611",
   "metadata": {},
   "source": [
    "How do the fake waveforms look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a347cf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "6a347cf2",
    "outputId": "05d75d51-5690-4e18-b2fb-638a42c13bbd"
   },
   "outputs": [],
   "source": [
    "fixed_noise = torch.normal(0, 1, size = (6, nz, 1)).cuda()\n",
    "test = generator(fixed_noise)\n",
    "show_some_cases(test.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YHVdlxW4knPj",
   "metadata": {
    "id": "YHVdlxW4knPj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
