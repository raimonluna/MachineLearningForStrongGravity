{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd65a04f",
   "metadata": {},
   "source": [
    "# Lectures on Machine Learning for Strong Gravity\n",
    "## Lecture 1: Physics-Informed Neural Networks\n",
    "\n",
    "To test it, simply press Ctrl+Enter sequentially in each cell, or click on the small icons on the left with the \"play\" symbol.\n",
    "\n",
    "<br>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/raimonluna/MachineLearningForStrongGravity/blob/main/Lecture1_Physics_Informed_Neural_Networks.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "### In this lecture, you will learn:\n",
    "1. How to use PINNs to solve differential equations, including ODEs, PDEs and eigenvalue problems.<br>Original paper:<br>\n",
    "\n",
    " - Maziar Raissi, Paris Perdikaris, George Em Karniadakis, <i>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</i>, Journal of Computational Physics, 378, 686-707, 2019. https://arxiv.org/abs/1711.10561\n",
    " \n",
    "2. How these can be used, for instance, for the computation of QNMs.<br>Original paper:<br>\n",
    "\n",
    " - Raimon Luna, Juan Calderón Bustillo, Juan José Seoane Martínez, Alejandro Torres-Forné, José A. Font, <i>Solving the Teukolsky equation with physics-informed neural networks\n",
    "</i>, Phys.Rev.D 107 (2023) 6, 064025, 2023. https://arxiv.org/abs/2212.06103\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f094b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd3d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(outputs, inputs, order = 1):\n",
    "    if order == 1:\n",
    "        return torch.autograd.grad(outputs, inputs, grad_outputs=torch.ones_like(outputs), create_graph=True)[0]\n",
    "    elif order > 1:\n",
    "        return gradients(gradients(outputs, inputs, 1), inputs, order - 1)\n",
    "    else:\n",
    "        return outputs\n",
    "\n",
    "def generate_2Dgrid(minimum1, maximum1, minimum2, maximum2, N):\n",
    "    grid1 = np.linspace(minimum1, maximum1, N, dtype = np.float32)\n",
    "    grid2 = np.linspace(minimum2, maximum2, N, dtype = np.float32)\n",
    "    x0, y0 = np.meshgrid(grid1, grid2)\n",
    "    x = torch.tensor(x0.reshape(N**2), requires_grad = True)\n",
    "    y = torch.tensor(y0.reshape(N**2), requires_grad = True)\n",
    "    return x, y\n",
    "\n",
    "def plot_form(x, y, z, N):\n",
    "    return map(lambda t: t.reshape(N,N).cpu().detach().numpy(), (x, y, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e49add8",
   "metadata": {},
   "source": [
    "# 1. Automatic differentiation and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9762f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(np.linspace(-2*np.pi, 2*np.pi, 100, dtype = np.float32)).reshape(100, 1)\n",
    "x.requires_grad = True\n",
    "\n",
    "y = torch.sin(x)\n",
    "\n",
    "dydx = gradients(y, x)\n",
    "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
    "plt.plot(x.detach().numpy(), dydx.detach().numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd465d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.5], requires_grad = True)\n",
    "y = torch.tensor([0.5], requires_grad = True)\n",
    "\n",
    "track = []\n",
    "\n",
    "optimizer = optim.Adam([x, y], lr=0.1)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = 1 - torch.exp( -(x + y)**2 - y**2  )\n",
    "    z.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    track.append([x.item(), y.item()])\n",
    "\n",
    "track = np.array(track)\n",
    "\n",
    "x0, y0 = np.meshgrid(np.linspace(-2,2,100), np.linspace(-2,2,100))\n",
    "plt.contourf(x0, y0, 1 - np.exp( -(x0 + y0)**2 - y0**2  ))\n",
    "plt.scatter(track[:, 0], track[:, 1], color = 'red', marker='x');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4977d7",
   "metadata": {},
   "source": [
    "## Challenges!\n",
    " - Try computing derivatives of other functions\n",
    " - Try changing the learning rate. What happens if it gets too small? Or too big?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be0f9dd",
   "metadata": {},
   "source": [
    "# 2. Solving a system of ODEs: Sine and Cosine\n",
    "\n",
    "Here we solve the system of ODEs\n",
    "\n",
    "$$s'(x) = c(x),$$\n",
    "$$c'(x) = s(x),$$\n",
    "\n",
    "with the initial conditions\n",
    "\n",
    "$$s(0) = 0, \\;c(0) = 1.$$\n",
    "\n",
    "We impose the initial conditions by weak enforcement.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a436eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ODE, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 2),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean = 0, std = 0.1)\n",
    "                nn.init.constant_(m.bias, val = 0.0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class ODELoss(nn.Module):\n",
    "    def __init__(self, ode):\n",
    "        super(ODELoss, self).__init__()\n",
    "        self.ode = ode\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        funcs  = self.ode(x)\n",
    "        \n",
    "        c, s = map(lambda i:  funcs[:,[i]], range(2))\n",
    "        dc = gradients(c, x)\n",
    "        ds = gradients(s, x)\n",
    "        \n",
    "        zero_vals = self.ode(torch.zeros(1,1))\n",
    "                \n",
    "        eq_loss = torch.mean((dc + s)**2 + (ds - c)**2)\n",
    "        ic_loss = (zero_vals[0,0] - 1)**2 + zero_vals[0,1]**2\n",
    "        \n",
    "        return eq_loss, ic_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb910439",
   "metadata": {},
   "outputs": [],
   "source": [
    "ode = ODE()\n",
    "odeloss = ODELoss(ode)\n",
    "loss_hist = []\n",
    "\n",
    "optimizer = optim.Adam(ode.parameters(), lr=1e-2)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=(0.05)**(1/4000))\n",
    "\n",
    "x = torch.tensor(np.linspace(0, 10, 100, dtype = np.float32)).reshape(100, 1)\n",
    "x.requires_grad = True\n",
    "\n",
    "################## Training and Plotting ##################\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 4));\n",
    "font = {'size'   : 19}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "for epoch in range(4000):\n",
    "    try:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        eq_loss, ic_loss = odeloss(x)\n",
    "        loss = eq_loss + ic_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            \n",
    "            ax1.cla()\n",
    "            ax1.set_xlabel('epoch')\n",
    "            ax1.set_ylabel('loss')\n",
    "            ax1.set_yscale('log')\n",
    "            ax1.plot(loss_hist)\n",
    "\n",
    "            ax2.cla()\n",
    "            ax2.set_xlabel('x')\n",
    "            ax2.set_ylabel('y')\n",
    "            ax2.plot(x.cpu().detach().numpy(), ode(x)[:, 0].cpu().detach().numpy())\n",
    "            ax2.plot(x.cpu().detach().numpy(), ode(x)[:, 1].cpu().detach().numpy())\n",
    "\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0096d96f",
   "metadata": {},
   "source": [
    "## Challenge!\n",
    "\n",
    " - Try imposing the conditions at $x = 2\\pi$ instead of at $x = 0$. Does it affect the performance?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1449f549",
   "metadata": {},
   "source": [
    "# 3. Simple Eigenvalue Problem: Vibrating String"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc08a99",
   "metadata": {},
   "source": [
    "Here we try to solve the eigenvalue problem\n",
    "\n",
    "$$f''(x) + \\lambda^2 f(x) = 0, \\quad f(0) = f(1) = 0.$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc68092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ODE, self).__init__()\n",
    "        \n",
    "        self.lam = nn.Parameter(torch.tensor(5.0, requires_grad = True))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 1),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean = 0, std = 0.1)\n",
    "                nn.init.constant_(m.bias, val = 0.0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.net(x) * x * (1 - x) \n",
    "    \n",
    "class ODELoss(nn.Module):\n",
    "    def __init__(self, ode):\n",
    "        super(ODELoss, self).__init__()\n",
    "        self.ode = ode\n",
    "        \n",
    "    def forward(self, x):\n",
    "        f  = self.ode(x)\n",
    "        d2f = gradients(f, x, 2)\n",
    "                \n",
    "        loss = torch.mean((d2f + f*self.ode.lam**2)**2) + (torch.mean(torch.abs(f)) - 1)**2\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ode = ODE()\n",
    "odeloss = ODELoss(ode)\n",
    "loss_hist = []\n",
    "\n",
    "optimizer = optim.Adam(ode.parameters(), lr=5e-3)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=(0.01)**(1/2000))\n",
    "x = torch.tensor(np.linspace(0, 1, 100, dtype = np.float32)).reshape(100, 1)\n",
    "x.requires_grad = True\n",
    "\n",
    "################## Training and Plotting ##################\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 6));\n",
    "font = {'size'   : 19}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    try:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = odeloss(x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            \n",
    "            plt.suptitle(f'$\\lambda = {ode.lam.item()}$')\n",
    "            \n",
    "            ax1.cla()\n",
    "            ax1.set_xlabel('epoch')\n",
    "            ax1.set_ylabel('loss')\n",
    "            ax1.set_yscale('log')\n",
    "            ax1.plot(loss_hist)\n",
    "            \n",
    "            ax2.cla()\n",
    "            ax2.set_xlabel('x')\n",
    "            ax2.set_ylabel('y')\n",
    "            ax2.plot(x.cpu().detach().numpy(), ode(x).cpu().detach().numpy())\n",
    "            \n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6737ccd",
   "metadata": {},
   "source": [
    "## Challenge!\n",
    "\n",
    " - Try solving the eiganvalue problem \n",
    "\n",
    "$$x^2 f''(x) + x f'(x) + \\lambda^2 x^2 f(x) = 0, \\quad f(0) = 1, \\; f(1) = 0$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f67888",
   "metadata": {},
   "source": [
    "# 4. Solving an Elliptic PDE: Poisson's Equation on a Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d1c36",
   "metadata": {},
   "source": [
    "Now we switch to a 2-dimensional case. Let's solve Poisson's equation \n",
    "\n",
    "$$\\nabla^2 u = - r^2 \\sin (\\theta / 2)^4 + \\sin(6\\theta) \\cos (\\theta / 2)^2,$$\n",
    "\n",
    "on a unit disk, with the boundary condition\n",
    "\n",
    "$$u(r=1) = 0$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PDE, self).__init__()\n",
    "        \n",
    "        self.lam = nn.Parameter(torch.tensor(1.0, requires_grad = True))\n",
    "                \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 1),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean = 0, std = 0.1)\n",
    "                nn.init.constant_(m.bias,  val = 0.0)\n",
    "                \n",
    "    def forward(self, r, th):\n",
    "        rth  = torch.stack((r**2, r**2 * torch.sin(th), r**2 * torch.cos(th)), 1)\n",
    "        bdry = (r**2 - 1).reshape(len(r), 1)\n",
    "        net_out = self.net(rth) * bdry\n",
    "        return net_out\n",
    "    \n",
    "    \n",
    "class PDELoss(nn.Module):\n",
    "    def __init__(self, pde):\n",
    "        super(PDELoss, self).__init__()\n",
    "        self.pde = pde\n",
    "        \n",
    "    def forward(self, r, th):\n",
    "        u   = self.pde(r, th)\n",
    "        u10 = gradients(u, r, 1)\n",
    "        u20 = gradients(u, r, 2)\n",
    "        u02 = gradients(u, th, 2)\n",
    "        \n",
    "        r2_lap_u = r**2 * u20 + r * u10 + u02\n",
    "        source   = - r**2 * torch.sin(th/2)**4 + torch.sin(6*th) * torch.cos(th/2)**2\n",
    "        \n",
    "        loss = torch.mean(torch.abs( r2_lap_u  - r**2 * source ))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pde      = PDE()\n",
    "pde_loss = PDELoss(pde)\n",
    "loss_hist = []\n",
    "\n",
    "r, th = generate_2Dgrid(0, 1, 0, 2*np.pi, 30)\n",
    "optimizer = optim.Adam(pde.parameters(), lr=1e-2)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=(0.1)**(1/5000))\n",
    "\n",
    "################## Training and Plotting ##################\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 5));\n",
    "font = {'size'   : 19}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "for it in range(5000):\n",
    "    try:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = pde_loss(r, th)\n",
    "        loss_hist.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "        if  it %10 == 0:\n",
    "            r_plot, th_plot, u_plot = plot_form(r, th, pde(r, th), 30)\n",
    "            \n",
    "            ax1.cla()\n",
    "            ax1.set_xlabel('epoch')\n",
    "            ax1.set_ylabel('loss')\n",
    "            ax1.set_yscale('log')\n",
    "            ax1.plot(loss_hist)\n",
    "            \n",
    "            ax2.cla()\n",
    "            ax2.set_xlabel('x')\n",
    "            ax2.set_ylabel('y')\n",
    "            ax2.contourf(r_plot * np.cos(th_plot), r_plot * np.sin(th_plot), u_plot)\n",
    "            \n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efdef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize = (5, 5))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "r, th = generate_2Dgrid(0.05, 1, 0, 2*np.pi, 100)\n",
    "r_plot, th_plot, u_plot = plot_form(r, th, pde(r, th), 100)\n",
    "\n",
    "ax.plot_wireframe(r_plot * np.cos(th_plot), r_plot * np.sin(th_plot), u_plot);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a8349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go back to regular plots!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2902d82",
   "metadata": {},
   "source": [
    "## Challenge!\n",
    " - How would you change the boundary condition to be \n",
    "\n",
    "$$u(r=1) = 0.01 \\sin(2\\theta)$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14892779",
   "metadata": {},
   "source": [
    "# 5. The Teukolsky Equation (Quasinormal Modes of Kerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2648d",
   "metadata": {},
   "source": [
    "Let's now find quasinormal modes of Kerr black holes. This part is based on https://arxiv.org/abs/2212.06103. In this case, the equation we try to solve is Teukolsky's equation. After separation of variables, we are left with\n",
    "\n",
    "$$\\Delta(r) R''(r) + (s+1) (2r -1)R'(r) +V(r)R(r)=0,$$\n",
    "\n",
    "$$(1-u^2)S''(u) - 2u S'(u) + \\left[a^2\\omega^2 u^2 - 2 a\\omega s u + s + A -\\frac{(m+su)^2}{1-u^2}\\right]S(u) = 0,\n",
    "$$\n",
    "\n",
    "where we defined\n",
    "\n",
    "$$\\Delta(r) = r^2 - r + a^2, \\qquad V(r) = \\alpha(r) \\omega^2 + \\beta(r) \\omega + \\gamma(r), $$\n",
    "\n",
    "$$\\alpha(r) = \\frac{(r^2+a^2)^2}{\\Delta(r)} - a^2, \\qquad \\beta(r) = \\frac{is (a^2 - r^2) - 2amr }{\\Delta(r)} + 2isr, \\qquad \\gamma(r) = \\frac{a^2 m^2 + i s a m (2r - 1)}{\\Delta(r)} - A. $$\n",
    "\n",
    "We define the functions $f(x)$ and $g(u)$, with $x = 1/r$ and $u = \\cos \\theta$, as\n",
    "\n",
    "$$R(r) = e^{i \\omega r} (r - r_-)^{p_-} (r - r_+)^{p_+} f(x),$$\n",
    "\n",
    "$$S(u) = e^{a \\omega u} (1 + u)^{|m-s|/2} (1 - u)^{|m+s|/2} g(u),$$\n",
    "\n",
    "with\n",
    "\n",
    "$$p_- = -1-s+i\\omega+i\\sigma_+, \\qquad p_+ = -s-i\\sigma_+, \\qquad \\sigma_+ = \\frac{\\omega r_+ - a m}{\\sqrt{1 - 4 a^2}}.$$\n",
    "\n",
    "We impose the normalization condition $f(1) = g(-1) = 1.$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycbc.waveform import ringdown as rd\n",
    "\n",
    "def gradients(outputs, inputs, order = 1):\n",
    "    \n",
    "    re_outputs = torch.real(outputs)\n",
    "    im_outputs = torch.imag(outputs)\n",
    "    if order == 1:\n",
    "        d_re = torch.autograd.grad(re_outputs, inputs, grad_outputs=torch.ones_like(re_outputs), create_graph=True)[0]\n",
    "        d_im = torch.autograd.grad(im_outputs, inputs, grad_outputs=torch.ones_like(im_outputs), create_graph=True)[0]\n",
    "        return d_re + (0 + 1j) * d_im\n",
    "    elif order > 1:\n",
    "        return gradients(gradients(outputs, inputs, 1), inputs, order - 1)\n",
    "    else:\n",
    "        return outputs\n",
    "\n",
    "class PyCBC_modes():\n",
    "    def __init__(self, l, m):\n",
    "        self.l = l\n",
    "        self.m = m\n",
    "        \n",
    "    def __call__(self, spin, n=0):\n",
    "        c  = 299792458\n",
    "        GM = 1.32712440018e20\n",
    "        freq, tau = rd.get_lm_f0tau_allmodes(1, 2*spin, [f'{self.l}{self.m}8'])\n",
    "        freq, tau = freq[f'{self.l}{self.m}{n}'], tau[f'{self.l}{self.m}{n}']\n",
    "        wreal =  freq * (4 * np.pi * GM) / c**3\n",
    "        wimag = - 1 / (tau * c**3 / (2 * GM))\n",
    "        return complex(wreal, wimag)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571c0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODE(nn.Module):\n",
    "    \n",
    "    def __init__(self, l, m):\n",
    "        super(ODE, self).__init__()\n",
    "        \n",
    "        # Spherical harmonic indices l and m\n",
    "        self.l = torch.tensor(l)\n",
    "        self.m = torch.tensor(m)\n",
    "        \n",
    "        #Initialize the eigenvalues omega and A_{lm} to sensible values\n",
    "        self.w   = nn.Parameter(torch.tensor(0.7 - 0.1j), requires_grad = True)\n",
    "        self.Alm = nn.Parameter(torch.tensor(l*(l+1) - 2.0 + 0.0j), requires_grad = True)\n",
    "        \n",
    "        #Net for R(x)\n",
    "        self.netR = nn.Sequential(\n",
    "            nn.Linear(1, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 2),\n",
    "        )\n",
    "        \n",
    "        #Net for S(x)\n",
    "        self.netS = nn.Sequential(\n",
    "            nn.Linear(1, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(40, 2),\n",
    "        )\n",
    "\n",
    "        #Random initialization of the netwok parameters\n",
    "        torch.manual_seed(42)\n",
    "        for m in self.netR.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean = 0, std = 0.05)\n",
    "                nn.init.constant_(m.bias, val = 0.0)\n",
    "                              \n",
    "        for m in self.netS.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean = 0, std = 0.01)\n",
    "                nn.init.constant_(m.bias, val = 0.0)\n",
    "                \n",
    "    def forward(self, x, u):\n",
    "        #Evaluate the neural networks and apply hard enforcement of normalization\n",
    "        outR = self.netR(x)\n",
    "        outS = self.netS(u)       \n",
    "        complex_resultR = outR[:,[0]] + 1j * outR[:,[1]]\n",
    "        complex_resultS = outS[:,[0]] + 1j * outS[:,[1]]\n",
    "        hard_enforceR   = (torch.exp(x - 1) - 1) \n",
    "        hard_enforceS   = (torch.exp(u + 1) - 1)\n",
    "        \n",
    "        return complex_resultR * hard_enforceR + 1, complex_resultS * hard_enforceS + 1\n",
    "    \n",
    "class ODELoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, ode, a):\n",
    "        super(ODELoss, self).__init__()\n",
    "        \n",
    "        self.ode = ode\n",
    "        self.a   = torch.tensor(a)\n",
    "        self.l   = ode.l\n",
    "        self.m   = ode.m\n",
    "        \n",
    "    def forward(self, x_0, u):\n",
    "        \n",
    "        #Recover eigenvalues\n",
    "        w   = self.ode.w\n",
    "        Alm = self.ode.Alm\n",
    "        \n",
    "        #Compute some common expressions\n",
    "        a = self.a\n",
    "        m = self.m\n",
    "        b = torch.sqrt(1 - 4*a**2)\n",
    "        abs1 = torch.abs(self.m + 2)\n",
    "        abs2 = torch.abs(self.m - 2)\n",
    "        \n",
    "        rp = (1 + b) / 2\n",
    "        \n",
    "        #Evaluate f, g, and their derivatives\n",
    "        f, g = self.ode(x_0, u)\n",
    "        df    = gradients(f,  x_0) #* rp\n",
    "        ddf   = gradients(df, x_0) #* rp**2\n",
    "        dg    = gradients(g,  u)\n",
    "        ddg   = gradients(dg, u)\n",
    "    \n",
    "        #Teukolsky master equations (power series part)\n",
    "        \n",
    "        x = x_0 #/ rp\n",
    "        \n",
    "        eqf = ddf*x**2*(1 + x*(-1 + a**2*x))**2 + df*(1 + x*(-1 + a**2*x))*(x*(-2 - (b + 2j*a*m)*x + 2*a**2*x**2) + 2j*w*(-1 + (1 - a**2 + b)*x**2)) + (f*(2*w*(1j*(3 + b) - 2*a*m - a**2*w + 2*(1 + b)*w) + 2*(b - 2j*(1 + b)*w + a**2*w**2 - 2*a*m*(1j + w))*x + (-4*a**3*m*w - 2*a**4*w**2 + 2*a*(1 + b)*m*(1j + 2*w) - (1 + b)*(1 + 4*w**2) + 2*a**2*(2 + 1j*(1 + b)*w + 2*(3 + b)*w**2))*x**2 - 2*Alm*(1 + x*(-1 + a**2*x))))/2.\n",
    "        eqg = g*(4*(2 + m**2 - 4*m*u + 2*u**2 + Alm*(-1 + u**2) - 2*a*u*w + 2*a*u**3*w - a**2*w**2 + a**2*u**2*w**2) - (1 + u)**2*abs2**2 - 2*(-1 + u**2)*(1 + 2*a*(-1 + u)*w)*abs1 - (-1 + u)**2*abs1**2 - 2*(-1 + u**2)*abs2*(1 + 2*a*(1 + u)*w + abs1)) - 4*(-1 + u**2)*(ddg*(-1 + u**2) + dg*(2*(u - a*w + a*u**2*w) + (1 + u)*abs2 + (-1 + u)*abs1))\n",
    "        \n",
    "        #Notice the radial and angular equations have weights of 10/1\n",
    "        eq_loss = 10*torch.mean(torch.abs(eqf)) + torch.mean(torch.abs(eqg))\n",
    "            \n",
    "        return eq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ea524",
   "metadata": {},
   "outputs": [],
   "source": [
    "l, m = 2, 0\n",
    "ode = ODE(l, m)\n",
    "right_modes = PyCBC_modes(l, m)\n",
    "\n",
    "#Definig the grid\n",
    "x = torch.tensor(np.linspace(0.0,  1.0, 100, dtype = np.float32)).reshape(100, 1)\n",
    "x.requires_grad = True\n",
    "u = torch.tensor(np.linspace(-1.0, 1.0, 100, dtype = np.float32)).reshape(100, 1)\n",
    "u.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can evaluate this cell on a pre-trained network\n",
    "a    = 0.45\n",
    "target_mode = right_modes(a)\n",
    "\n",
    "loss_hist = []\n",
    "odeloss   = ODELoss(ode, a)\n",
    "optimizer = optim.Adam(ode.parameters(), lr = 0.005)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figure with subplots of different sizes\n",
    "fig = plt.figure(figsize = (12, 8))\n",
    "font = {'size'   : 18}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# set up subplot grid\n",
    "gridspec.GridSpec(2,4)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    try:\n",
    "        #Training step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = odeloss(x, u)\n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            \n",
    "            #Visualization\n",
    "            f, g = ode(x, u)\n",
    "            \n",
    "            plt.suptitle(f'$J/M$ = {2*a},    ($l, m$) = ({l},{m}),\\n' + \\\n",
    "                         f'$\\omega$ = {np.real(ode.w.item()):.5} - {-np.imag(ode.w.item()):.5} i,    ' + \\\n",
    "                         f'$A$ = {np.real(ode.Alm.item()):.5} + {np.imag(ode.Alm.item()):.5} i \\n' + \\\n",
    "                         f'Target Value: $\\omega_0$ = {np.real(target_mode):.5} - {-np.imag(target_mode):.5} i' )\n",
    "\n",
    "            plt.subplot2grid((2,4), (0,0), colspan=2, rowspan=1)\n",
    "            plt.cla()\n",
    "            plt.plot(x.cpu().detach().numpy(), np.real(f.cpu().detach().numpy()), label = '$Re\\; [f]$')\n",
    "            plt.plot(x.cpu().detach().numpy(), np.imag(f.cpu().detach().numpy()), label = '$Im\\; [f]$')\n",
    "            plt.grid()\n",
    "            #plt.legend(loc = 'upper left')\n",
    "            plt.xlabel('$x$')\n",
    "            plt.ylabel('$f(x)$')\n",
    "            \n",
    "            plt.subplot2grid((2,4), (1,0), colspan=2, rowspan=1)\n",
    "            plt.cla()\n",
    "            plt.plot(u.cpu().detach().numpy(), np.real(g.cpu().detach().numpy()), label = '$Re\\; [g]$')\n",
    "            plt.plot(u.cpu().detach().numpy(), np.imag(g.cpu().detach().numpy()), label = '$Im\\; [g]$')\n",
    "            plt.grid()\n",
    "            #plt.legend(loc = 'center left')\n",
    "            plt.xlabel('$u$')\n",
    "            plt.ylabel('$g(u)$')\n",
    "            \n",
    "            plt.subplot2grid((2,4), (0,2), colspan=2, rowspan=2)\n",
    "            plt.cla()\n",
    "            plt.yscale('log')\n",
    "            plt.plot(loss_hist)\n",
    "            plt.grid()\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            \n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            plt.tight_layout()\n",
    "                        \n",
    "    except KeyboardInterrupt:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428690d",
   "metadata": {},
   "source": [
    "## Challenges!\n",
    " - Try changing the angular momentum parameter $a$, and see how the mode changes.\n",
    " - Can you get a higher mode? Say, the (2,2,1) mode?\n",
    " - What happens if you train on a smaller $x$ range?\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66f6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
